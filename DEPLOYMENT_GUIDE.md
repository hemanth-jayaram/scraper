# üöÄ Ultimate Scraper V2 - Web Interface Deployment Guide

**A comprehensive guide for deploying the Ultimate Scraper V2 Web Interface to production environments.**

---

## Table of Contents

1. [Environment Variables and Configuration](#1-environment-variables-and-configuration)
2. [Setup Instructions](#2-setup-instructions)
3. [Deployment Steps](#3-deployment-steps)
4. [Validation and Testing](#4-validation-and-testing)
5. [Common Pitfalls](#5-common-pitfalls)
6. [Changing EC2 Instance](#6-changing-ec2-instance)

---

## Project Overview

The **Ultimate Scraper V2 Web Interface** is a **completely self-contained** Flask-based web application that provides a beautiful, modern interface for managing EC2-based article scraping jobs. This project includes everything needed for deployment with no external dependencies.

**‚úÖ SELF-CONTAINED PROJECT STRUCTURE:**
- **Web Server** (`web_server.py`) - Flask backend with real-time progress tracking and full S3 integration
- **Main Scraper** (`ultimate_scraper_v2.py`) - The core scraping engine that runs on EC2 (now with relative paths)
- **Web Interface** (`index.html`) - Modern responsive frontend
- **Launcher** (`launch_scraper_interface.bat`) - One-click Windows launcher with enhanced error handling
- **Configuration** (`.env`, `s3_config.example.env`) - All S3 and AWS configuration files included
- **Dependencies** (`requirements_web.txt`) - All Python packages including boto3 for S3
- **Documentation** (`README.md`) - Updated project documentation

**üéØ DEPLOYMENT READY:** This folder can be copied anywhere and deployed independently!

---

## üöÄ **SELF-CONTAINED PROJECT NOTICE**

**‚úÖ IMPORTANT**: This is now a **completely self-contained project**. All changes have been made to ensure:

### ‚úÖ **What's Included (All Dependencies)**
- ‚úÖ **All configuration files** (`.env`, `s3_config.example.env`)
- ‚úÖ **All Python dependencies** (`requirements_web.txt` with boto3)
- ‚úÖ **Enhanced launcher** with improved error handling
- ‚úÖ **Relative paths** in scraper (no hardcoded absolute paths)
- ‚úÖ **Full S3 integration** in web server
- ‚úÖ **Updated documentation** (this guide + README.md)

### ‚úÖ **What This Means**
- üìÅ **Portable**: Copy the `final/` folder anywhere - it will work
- üöÄ **Independent**: No external dependencies outside this folder
- üîß **Complete**: Everything needed for deployment is included
- üéØ **Production Ready**: Can be deployed immediately

### ‚úÖ **Deployment Options**
1. **Copy to new server**: Just copy the entire `final/` folder
2. **Local development**: Run directly from this folder
3. **Team deployment**: Share this folder - everything is included
4. **Clean deployment**: Can delete everything else except `final/` folder

---

## 1. Environment Variables and Configuration

### 1.1 üéØ **COMPLETE CODE CHANGE GUIDE**

**This section provides EXACT locations in the code where deployment teams need to make changes for different EC2 instances and S3 buckets.**

---

#### **üìÅ FILE: `web_server.py` - Main Flask Backend**

**üîß EC2 Configuration (Lines 30-34):**
```python
# CURRENT CODE (Lines 30-34):
EC2_HOST = "54.82.140.246"                                    # ‚ö†Ô∏è CHANGE THIS
EC2_USER = "ec2-user" 
EC2_KEY_PATH = r"C:\Users\heman\Downloads\key-scraper.pem"    # ‚ö†Ô∏è CHANGE THIS
EC2_SCRAPER_PATH = "/home/ec2-user/ultimate_scraper_v2.py"    # ‚úÖ Keep as-is
EC2_ENV_PATH = "/home/ec2-user/ultimate_scraper_env/bin/activate" # ‚úÖ Keep as-is

# EXAMPLE CHANGE FOR NEW EC2:
EC2_HOST = "18.234.567.890"                                   # üëà Your new EC2 IP
EC2_USER = "ec2-user" 
EC2_KEY_PATH = r"C:\path\to\your\new-key.pem"                # üëà Your new key file
```

**üîß S3 Configuration (Line 37):**
```python
# CURRENT CODE (Line 37):
S3_BUCKET_NAME = os.getenv('S3_BUCKET_NAME', 'bockscraper')  # ‚ö†Ô∏è Default bucket name

# EXAMPLE CHANGE FOR NEW S3 BUCKET:
S3_BUCKET_NAME = os.getenv('S3_BUCKET_NAME', 'my-new-bucket') # üëà Your new bucket
```

---

#### **üìÅ FILE: `.env` - Environment Variables**

**üîß S3 Bucket Configuration (Line 7):**
```bash
# CURRENT CODE (Line 7):
S3_BUCKET_NAME=bockscraper                                    # ‚ö†Ô∏è CHANGE THIS

# EXAMPLE CHANGE:
S3_BUCKET_NAME=my-new-production-bucket                      # üëà Your bucket name
```

**üîß AWS Credentials (Lines 13-18):**
```bash
# CURRENT CODE (Lines 13-18):
# AWS_ACCESS_KEY_ID=your_access_key_here                     # ‚ö†Ô∏è UNCOMMENT & CHANGE
# AWS_SECRET_ACCESS_KEY=your_secret_key_here                 # ‚ö†Ô∏è UNCOMMENT & CHANGE
# AWS_DEFAULT_REGION=us-east-1                               # ‚ö†Ô∏è UNCOMMENT & CHANGE

# EXAMPLE CHANGE:
AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE                       # üëà Your access key
AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY # üëà Your secret
AWS_DEFAULT_REGION=us-west-2                                 # üëà Your region
```

---

#### **üìÅ FILE: `launch_scraper_interface.bat` - Windows Launcher**

**üîß EC2 Connection Test (Line 66):**
```batch
REM CURRENT CODE (Line 66):
echo Checking connection to 54.82.140.246...                # ‚ö†Ô∏è CHANGE THIS IP

REM EXAMPLE CHANGE:
echo Checking connection to 18.234.567.890...               # üëà Your new EC2 IP
```

---

### 1.2 üìã **QUICK REFERENCE TABLE**

| **File** | **Lines** | **What to Change** | **Example** |
|----------|-----------|-------------------|-------------|
| `web_server.py` | 30 | EC2 IP Address | `"18.234.567.890"` |
| `web_server.py` | 32 | SSH Key Path | `r"C:\keys\prod-key.pem"` |
| `web_server.py` | 37 | Default S3 Bucket | `'my-prod-bucket'` |
| `.env` | 7 | S3 Bucket Name | `S3_BUCKET_NAME=my-prod-bucket` |
| `.env` | 13-15 | AWS Credentials | Uncomment and add real values |
| `launch_scraper_interface.bat` | 66 | EC2 IP for testing | Your production IP |

---

### 1.3 üö® **CRITICAL: Deployment Team Instructions**

**üéØ EXACTLY what deployment teams need to change for different environments:**

#### **Scenario A: Switching to New EC2 Instance**
**Files to Edit: 2 files**

1. **File**: `web_server.py` 
   - **Line 30**: Change `EC2_HOST = "54.82.140.246"` to your new IP
   - **Line 32**: Change `EC2_KEY_PATH = r"C:\Users\heman\Downloads\key-scraper.pem"` to your new key path

2. **File**: `launch_scraper_interface.bat`
   - **Line 66**: Change `54.82.140.246` to your new EC2 IP

#### **Scenario B: Switching to New S3 Bucket**
**Files to Edit: 2 files**

1. **File**: `.env`
   - **Line 7**: Change `S3_BUCKET_NAME=bockscraper` to your new bucket name

2. **File**: `web_server.py` (Optional)
   - **Line 37**: Change default bucket name in fallback

#### **Scenario C: New AWS Account/Credentials**
**Files to Edit: 1 file**

1. **File**: `.env`
   - **Lines 13-15**: Uncomment and add your AWS credentials

---

### 1.4 Required Configuration Changes

#### **A. Update Web Server Configuration (`web_server.py`)**

**Lines 28-32: EC2 Connection Settings**
```python
# BEFORE (current values):
EC2_HOST = "54.82.140.246"                                    # CHANGE THIS
EC2_USER = "ec2-user"                                         # Keep as-is
EC2_KEY_PATH = r"C:\Users\heman\Downloads\key-scraper.pem"   # CHANGE THIS
EC2_SCRAPER_PATH = "/home/ec2-user/ultimate_scraper_v2.py"    # Keep as-is
EC2_ENV_PATH = "/home/ec2-user/ultimate_scraper_env/bin/activate"  # Keep as-is

# AFTER (your production values):
EC2_HOST = "YOUR_PRODUCTION_EC2_IP"                           # Your EC2 public IP
EC2_USER = "ec2-user"                                         # Keep as-is
EC2_KEY_PATH = r"C:\path\to\your\production-key.pem"        # Your SSH key path
EC2_SCRAPER_PATH = "/home/ec2-user/ultimate_scraper_v2.py"    # Keep as-is
EC2_ENV_PATH = "/home/ec2-user/ultimate_scraper_env/bin/activate"  # Keep as-is
```

#### **B. Update Launcher Script (`launch_scraper_interface.bat`)**

**Line 66: EC2 Connection Test**
```batch
REM BEFORE:
echo Checking connection to 54.82.140.246...

REM AFTER:
echo Checking connection to YOUR_PRODUCTION_EC2_IP...
```

### 1.3 S3 Configuration Required

This **self-contained** web interface project **DOES require S3 setup** for the complete workflow:
- ‚úÖ S3 bucket configuration
- ‚úÖ AWS credentials setup  
- ‚úÖ Environment variables for S3 access
- ‚úÖ S3 bucket name configuration

**‚úÖ SELF-CONTAINED S3 SETUP**: All S3 configuration files are now included in this project folder (`.env`, `s3_config.example.env`).

**Workflow**: Articles are scraped on EC2 ‚Üí Uploaded to S3 ‚Üí Downloaded to local machine via web interface

#### **A. S3 Configuration File (`.env`)**
```bash
# S3 Configuration - CHANGE THESE VALUES
S3_UPLOAD_ENABLED=true
S3_BUCKET_NAME=your-production-bucket-name
AWS_REGION=us-east-1

# AWS Credentials - UNCOMMENT AND UPDATE
AWS_ACCESS_KEY_ID=your-access-key-id
AWS_SECRET_ACCESS_KEY=your-secret-access-key
```

#### **B. Web Server S3 Configuration (`web_server.py`)**
```python
# Line 37: Update S3 bucket name
S3_BUCKET_NAME = os.getenv('S3_BUCKET_NAME', 'your-production-bucket-name')
```

---

## 2. Setup Instructions

### 2.1 EC2 Instance Setup

#### **A. Launch EC2 Instance**
1. **AMI**: Amazon Linux 2 or Amazon Linux 2023
2. **Instance Type**: `t3.medium` or larger (minimum 2 vCPU, 4GB RAM)
3. **Security Group**: 
   - SSH (port 22) from your deployment machine IP
   - **No need for port 5000** - web server runs locally
4. **Key Pair**: Download and save the `.pem` file securely
5. **Storage**: 20GB+ EBS volume

#### **B. Install Dependencies on EC2**
```bash
# Connect to EC2
ssh -i /path/to/your-key.pem ec2-user@YOUR_EC2_IP

# Update system
sudo yum update -y

# Install Python 3 and development tools
sudo yum install -y python3 python3-pip python3-devel gcc

# Install required system packages for image processing
sudo yum install -y libjpeg-devel zlib-devel freetype-devel

# Create project directory
mkdir -p ~/
cd ~/
```

#### **C. Upload Scraper Script to EC2**
```bash
# From your local machine (Windows PowerShell or Command Prompt)
scp -i "C:\path\to\your-key.pem" "C:\path\to\final\ultimate_scraper_v2.py" ec2-user@YOUR_EC2_IP:~/

# Verify upload
ssh -i "C:\path\to\your-key.pem" ec2-user@YOUR_EC2_IP "ls -la ~/ultimate_scraper_v2.py"
```

#### **D. Setup Python Environment on EC2**
```bash
# On EC2 instance
cd ~/

# Create virtual environment
python3 -m venv ultimate_scraper_env
source ultimate_scraper_env/bin/activate

# Upgrade pip
pip install --upgrade pip

# Install required packages for ultimate_scraper_v2.py
pip install scrapy>=2.11.0
pip install trafilatura>=1.6.0
pip install beautifulsoup4>=4.12.0
pip install lxml>=4.9.0
pip install newspaper3k>=0.2.8
pip install Pillow>=9.0.0
pip install aiohttp>=3.8.0
pip install requests>=2.28.0

# Test the scraper installation
python ultimate_scraper_v2.py --help
```

### 2.2 Local Machine Setup (Windows)

#### **A. Prerequisites**
- Windows 10/11 with PowerShell
- Python 3.8+ installed and in PATH
- SSH client available (built into Windows 10/11)

#### **B. Verify Python Installation**
```cmd
# Open Command Prompt or PowerShell
python --version
# Should show Python 3.8.0 or higher
```

#### **C. Download Project Files**
Ensure you have all files from the `final` folder:
- `web_server.py`
- `ultimate_scraper_v2.py` 
- `index.html`
- `requirements_web.txt`
- `launch_scraper_interface.bat`
- `README.md`

---

## 3. Deployment Steps

### 3.1 Pre-Deployment Checklist

- [ ] EC2 instance is running and accessible via SSH
- [ ] SSH key file (.pem) has correct permissions and path is noted
- [ ] `ultimate_scraper_v2.py` is uploaded to EC2 home directory
- [ ] Python virtual environment is created on EC2 with required packages
- [ ] Local machine has Python 3.8+ installed
- [ ] All configuration files are updated with production values

### 3.2 Update Configuration Files

#### **Step 1: Update Web Server Configuration**
```bash
# Edit web_server.py with your production values
# Lines 28-32: Update EC2 connection details

# Example using notepad:
notepad web_server.py

# Or using VS Code:
code web_server.py
```

**Required Changes:**
```python
# Line 28: Change EC2 IP address
EC2_HOST = "YOUR_PRODUCTION_EC2_IP"

# Line 30: Change SSH key path  
EC2_KEY_PATH = r"C:\path\to\your\production-key.pem"
```

#### **Step 2: Update Launcher Script**
```bash
# Edit launch_scraper_interface.bat
notepad launch_scraper_interface.bat

# Line 66: Update the connection test IP
echo Checking connection to YOUR_PRODUCTION_EC2_IP...
```

#### **Step 3: Verify SSH Key Permissions**
```cmd
# On Windows, ensure the SSH key file is secure
# Right-click the .pem file ‚Üí Properties ‚Üí Security
# Remove all users except your account and SYSTEM
# Give your account Full Control
```

### 3.3 Test EC2 Connection

#### **Step 1: Manual SSH Test**
```cmd
# Test SSH connection manually
ssh -i "C:\path\to\your-key.pem" ec2-user@YOUR_EC2_IP

# If successful, you should see the EC2 command prompt
# Test the scraper:
ls -la ~/ultimate_scraper_v2.py
source ~/ultimate_scraper_env/bin/activate
python ~/ultimate_scraper_v2.py --help

# Exit EC2
exit
```

#### **Step 2: Test Scraper Execution**
```cmd
# Test a quick scrape to verify everything works
ssh -i "C:\path\to\your-key.pem" ec2-user@YOUR_EC2_IP "source ~/ultimate_scraper_env/bin/activate && python ~/ultimate_scraper_v2.py 'https://www.bbc.com/news' --max-articles 2 --output test_output"
```

### 3.4 Deploy Web Interface

#### **Step 1: Launch Web Interface**
```cmd
# Navigate to the final folder
cd C:\path\to\your\final\folder

# Run the launcher (this will set up everything automatically)
launch_scraper_interface.bat
```

The launcher script will:
1. ‚úÖ Check Python installation
2. ‚úÖ Create virtual environment (`web_venv`)
3. ‚úÖ Install Flask dependencies
4. ‚úÖ Test EC2 connection
5. ‚úÖ Launch the web server
6. ‚úÖ Open browser to `http://localhost:5000`

#### **Step 2: Alternative Manual Launch**
```cmd
# If you prefer manual setup:
cd C:\path\to\your\final\folder

# Create virtual environment
python -m venv web_venv
web_venv\Scripts\activate

# Install dependencies
pip install -r requirements_web.txt

# Start web server
python web_server.py
```

### 3.5 Production Deployment Options

#### **Option A: Windows Service (Advanced)**
```cmd
# Install NSSM (Non-Sucking Service Manager)
# Download from: https://nssm.cc/download

# Create Windows service
nssm install "UltimateScraperWeb" "C:\path\to\final\web_venv\Scripts\python.exe" "C:\path\to\final\web_server.py"
nssm set "UltimateScraperWeb" AppDirectory "C:\path\to\final"
nssm start "UltimateScraperWeb"
```

#### **Option B: Task Scheduler (Simple)**
```cmd
# Create a scheduled task that runs at startup
# Task Scheduler ‚Üí Create Basic Task
# Name: Ultimate Scraper Web Interface
# Trigger: When the computer starts
# Action: Start a program
# Program: C:\path\to\final\launch_scraper_interface.bat
```

---

## 4. Validation and Testing

### 4.1 Test Self-Contained Setup

#### **A. Verify Project Completeness**
```cmd
# Navigate to the final folder
cd C:\path\to\final

# Check all required files are present
dir
```

**Expected self-contained files:**
- ‚úÖ `.env` - S3 configuration
- ‚úÖ `DEPLOYMENT_GUIDE.md` - This guide
- ‚úÖ `index.html` - Web interface
- ‚úÖ `launch_scraper_interface.bat` - Enhanced launcher
- ‚úÖ `README.md` - Updated project documentation
- ‚úÖ `requirements_web.txt` - Python dependencies (includes boto3)
- ‚úÖ `s3_config.example.env` - Configuration template
- ‚úÖ `ultimate_scraper_v2.py` - Core scraper (now with relative paths)
- ‚úÖ `web_server.py` - Flask backend with full S3 integration

#### **B. Launch Test**
```cmd
# Run the enhanced launcher
launch_scraper_interface.bat
```

**Expected Output:**
```
===============================================
   üöÄ ULTIMATE SCRAPER V2 WEB INTERFACE üöÄ
===============================================

  Modern ‚Ä¢ Beautiful ‚Ä¢ Real-time ‚Ä¢ Cloud-powered

===============================================
‚úÖ Python detected
üì¶ Creating virtual environment for web interface...
‚úÖ Virtual environment created
üîÑ Activating virtual environment...
üì• Installing/updating web interface dependencies...
‚úÖ Dependencies ready
üîç Testing EC2 connection...
Checking connection to YOUR_EC2_IP...
üöÄ Starting Flask web server...
```

#### **B. Browser Test**
1. **Browser should open automatically** to `http://localhost:5000`
2. **Verify interface loads** with modern design
3. **Check "Test EC2 Connection"** button works
4. **Expected response**: `"EC2 connection successful"`

### 4.2 Test Complete Scraping Pipeline

#### **A. Quick Test Scrape**
1. **Open web interface**: `http://localhost:5000`
2. **Enter test URL**: `https://www.bbc.com/news`
3. **Set max articles**: `5` (for quick testing)
4. **Set output path**: `test_results`
5. **Click "Start Scraping"**

#### **B. Monitor Progress**
Watch for these status updates:
```
Connecting to EC2...           (Progress: 5%)
Connected to EC2 successfully! (Progress: 10%)
Preparing scraper...           (Progress: 15%)
Scraping in progress...        (Progress: 15-85%)
Downloading results...         (Progress: 85-100%)
Completed! Files saved to...   (Progress: 100%)
```

#### **C. Verify Results**
```cmd
# Check that files were downloaded locally
dir test_results
# Should show article folders with images and JSON files
```

### 4.3 Test EC2 Connection Endpoint

#### **A. Direct API Test**
```cmd
# Test the connection endpoint directly
curl http://localhost:5000/test_connection

# Expected response:
{"status": "connected", "message": "EC2 connection successful"}
```

#### **B. Web Interface Connection Test**
1. **Click "Test EC2 Connection"** in the web interface
2. **Green success message** should appear
3. **Check browser console** for any errors (F12 ‚Üí Console)

### 4.4 Verify Log Files

#### **A. Check Web Server Logs**
```cmd
# Web server logs are displayed in the console
# Look for these key messages:
Web server starting...
Ultimate Scraper V2 Web Interface Ready
EC2 Instance: YOUR_EC2_IP
üåê Open your browser and go to: http://localhost:5000
```

#### **B. Check Scraper Logs on EC2**
```cmd
# SSH to EC2 and check for any log files
ssh -i "C:\path\to\your-key.pem" ec2-user@YOUR_EC2_IP
ls -la ~/*.log
ls -la ~/scraping_output_*/
```

---

## 5. Common Pitfalls

### 5.1 Connection Issues

#### **Problem: "SSH connection failed"**
**Causes & Solutions:**

**A. Security Group Issues**
```cmd
# Check EC2 security group allows SSH from your IP
# AWS Console ‚Üí EC2 ‚Üí Security Groups ‚Üí Inbound Rules
# Required: SSH (port 22) from YOUR_PUBLIC_IP/32
```

**B. SSH Key Permissions (Windows)**
```cmd
# Fix Windows SSH key permissions:
# 1. Right-click .pem file ‚Üí Properties ‚Üí Security
# 2. Click "Advanced" ‚Üí "Disable inheritance"
# 3. Remove all users except your account
# 4. Give your account "Full control"
```

**C. Wrong IP Address**
```cmd
# Verify you're using the correct public IP
# AWS Console ‚Üí EC2 ‚Üí Instances ‚Üí Select instance ‚Üí Public IPv4 address
# Or use AWS CLI:
aws ec2 describe-instances --instance-ids i-1234567890abcdef0 --query 'Reservations[0].Instances[0].PublicIpAddress'
```

#### **Problem: "Connection timeout"**
**Causes & Solutions:**

**A. Instance Not Running**
```cmd
# Check instance state in AWS Console
# Start the instance if it's stopped
aws ec2 start-instances --instance-ids i-1234567890abcdef0
```

**B. Network ACLs or VPC Issues**
```cmd
# Ensure instance is in a public subnet
# Verify internet gateway and route table configuration
# Check Network ACLs allow SSH traffic
```

### 5.2 Web Interface Issues

#### **Problem: "Web interface not loading"**
**Causes & Solutions:**

**A. Python Not Installed**
```cmd
# Install Python 3.8+ from python.org
# Verify installation:
python --version
```

**B. Port 5000 Already in Use**
```cmd
# Check what's using port 5000:
netstat -ano | findstr :5000

# Kill the process if needed:
taskkill /PID [PID_NUMBER] /F

# Or change port in web_server.py (line 373):
app.run(host='0.0.0.0', port=5001, debug=False, threaded=True)
```

**C. Firewall Blocking Local Access**
```cmd
# Add Windows Firewall exception:
# Control Panel ‚Üí System and Security ‚Üí Windows Defender Firewall
# ‚Üí Allow an app through firewall ‚Üí Allow Python
```

#### **Problem: "Module not found errors"**
**Causes & Solutions:**

**A. Virtual Environment Issues**
```cmd
# Delete and recreate virtual environment:
rmdir /s web_venv
python -m venv web_venv
web_venv\Scripts\activate
pip install -r requirements_web.txt
```

**B. Missing Dependencies**
```cmd
# Install dependencies manually:
pip install Flask>=2.3.0
pip install flask-cors>=4.0.0
pip install paramiko>=3.3.0
pip install pathlib2>=2.3.0
```

### 5.3 EC2 Scraper Issues

#### **Problem: "Scraper script not found on EC2"**
**Causes & Solutions:**

**A. File Not Uploaded**
```cmd
# Re-upload the scraper script:
scp -i "C:\path\to\key.pem" ultimate_scraper_v2.py ec2-user@YOUR_EC2_IP:~/

# Verify upload:
ssh -i "C:\path\to\key.pem" ec2-user@YOUR_EC2_IP "ls -la ~/ultimate_scraper_v2.py"
```

**B. Wrong File Permissions**
```cmd
# Make script executable:
ssh -i "C:\path\to\key.pem" ec2-user@YOUR_EC2_IP "chmod +x ~/ultimate_scraper_v2.py"
```

#### **Problem: "Virtual environment not found"**
**Causes & Solutions:**

**A. Environment Not Created**
```cmd
# Create virtual environment on EC2:
ssh -i "C:\path\to\key.pem" ec2-user@YOUR_EC2_IP
python3 -m venv ~/ultimate_scraper_env
source ~/ultimate_scraper_env/bin/activate
pip install --upgrade pip
pip install scrapy trafilatura beautifulsoup4 lxml newspaper3k Pillow aiohttp requests
```

**B. Wrong Path in Configuration**
```cmd
# Verify the virtual environment path in web_server.py line 32:
EC2_ENV_PATH = "/home/ec2-user/ultimate_scraper_env/bin/activate"

# Check actual path on EC2:
ssh -i "C:\path\to\key.pem" ec2-user@YOUR_EC2_IP "ls -la ~/ultimate_scraper_env/bin/activate"
```

### 5.4 Scraping Performance Issues

#### **Problem: "Slow scraping or timeouts"**
**Causes & Solutions:**

**A. Insufficient EC2 Resources**
```cmd
# Upgrade to larger instance type:
# t3.medium ‚Üí t3.large or t3.xlarge
# More CPU and memory = faster processing
```

**B. Network Bandwidth Limits**
```cmd
# Consider instances with better network performance:
# t3.medium: Up to 5 Gbps
# t3.large: Up to 5 Gbps  
# t3.xlarge: Up to 5 Gbps
# m5.large: Up to 10 Gbps
```

**C. Concurrent Processing Limits**
```cmd
# Reduce concurrency in web interface if causing issues
# Default: 50 concurrent requests
# Try: 20-30 for better stability
```

### 5.5 Local File Download Issues

#### **Problem: "Files not downloading to local machine"**
**Causes & Solutions:**

**A. SSH Connection Lost During Transfer**
```cmd
# Check SSH connection stability
# Increase SSH timeout in web_server.py line 93:
self.ssh_client.connect(
    hostname=EC2_HOST,
    username=EC2_USER,
    key_filename=EC2_KEY_PATH,
    timeout=60  # Increase from 30 to 60 seconds
)
```

**B. Local Disk Space Issues**
```cmd
# Check available disk space:
dir C:\ 
# Ensure sufficient space for downloaded results
# Each article with image: ~1-5MB
```

**C. File Permissions on Windows**
```cmd
# Ensure output directory is writable:
# Right-click output folder ‚Üí Properties ‚Üí Security
# Verify your user has "Full control"
```

---

## üéØ **Quick Deployment Summary**

### **Essential Changes Required:**
1. **Update IP Address**: Change `54.82.140.246` to your EC2 IP (2 files)
2. **Update SSH Key Path**: Change key path to your production key (1 file)
3. **Upload Scraper**: Copy `ultimate_scraper_v2.py` to EC2 home directory
4. **Setup EC2 Environment**: Create virtual env and install packages on EC2

### **Critical Files to Update:**
- `web_server.py` (lines 28, 30)
- `launch_scraper_interface.bat` (line 66)

### **Deployment Commands:**
```cmd
# 1. Update configuration files locally (EC2 IP and SSH key path)

# 2. Upload scraper to EC2
scp -i "C:\path\to\key.pem" ultimate_scraper_v2.py ec2-user@YOUR_EC2_IP:~/

# 3. Setup EC2 environment
ssh -i "C:\path\to\key.pem" ec2-user@YOUR_EC2_IP
python3 -m venv ~/ultimate_scraper_env
source ~/ultimate_scraper_env/bin/activate
pip install scrapy trafilatura beautifulsoup4 lxml newspaper3k Pillow aiohttp requests

# 4. Launch web interface locally
cd C:\path\to\final
launch_scraper_interface.bat
```

### **Testing Checklist:**
- [ ] SSH connection to EC2 works
- [ ] Scraper script exists on EC2: `~/ultimate_scraper_v2.py`
- [ ] Virtual environment works on EC2
- [ ] Web interface loads at `http://localhost:5000`
- [ ] "Test EC2 Connection" button shows success
- [ ] Test scrape with 2-3 articles completes successfully
- [ ] Files download to local output directory

**üöÄ Your Ultimate Scraper V2 Web Interface is now ready for production use!**

---

## üìã **File Structure After Deployment**

```
C:\path\to\final\
‚îú‚îÄ‚îÄ web_server.py              (Updated with your EC2 IP)
‚îú‚îÄ‚îÄ ultimate_scraper_v2.py     (Also copied to EC2)
‚îú‚îÄ‚îÄ index.html                 (Web interface - no changes needed)
‚îú‚îÄ‚îÄ requirements_web.txt       (Dependencies - no changes needed)
‚îú‚îÄ‚îÄ launch_scraper_interface.bat (Updated with your EC2 IP)
‚îú‚îÄ‚îÄ README.md                  (Documentation)
‚îú‚îÄ‚îÄ DEPLOYMENT_GUIDE.md        (This file)
‚îî‚îÄ‚îÄ web_venv\                  (Created by launcher script)
    ‚îú‚îÄ‚îÄ Scripts\
    ‚îî‚îÄ‚îÄ Lib\

EC2 Instance (~/):
‚îú‚îÄ‚îÄ ultimate_scraper_v2.py     (Uploaded from local)
‚îî‚îÄ‚îÄ ultimate_scraper_env\      (Created during setup)
    ‚îú‚îÄ‚îÄ bin\
    ‚îî‚îÄ‚îÄ lib\
```

The system is designed to be **simple and reliable** - update the configuration files, set up S3 access, upload one file to EC2, and launch!

---

## üéØ **DEPLOYMENT TEAM QUICK REFERENCE**

### **üìã Complete Checklist: What Files to Change**

#### **For EC2 Instance Changes:**
- [ ] **File**: `web_server.py` **Line 30**: Update `EC2_HOST = "YOUR_NEW_IP"`
- [ ] **File**: `web_server.py` **Line 32**: Update `EC2_KEY_PATH = r"C:\path\to\new-key.pem"`
- [ ] **File**: `launch_scraper_interface.bat` **Line 66**: Update IP in connection test

#### **For S3 Bucket Changes:**
- [ ] **File**: `.env` **Line 7**: Update `S3_BUCKET_NAME=your-new-bucket`
- [ ] **File**: `web_server.py` **Line 37**: (Optional) Update default bucket name

#### **For AWS Credentials Changes:**
- [ ] **File**: `.env` **Lines 13-15**: Uncomment and add credentials

### **üîç Exact Search Terms for Code Editors**

**When using Find/Replace in your code editor:**

| **What to Find** | **Replace With** | **File** |
|------------------|------------------|----------|
| `EC2_HOST = "54.82.140.246"` | `EC2_HOST = "YOUR_NEW_IP"` | `web_server.py` |
| `EC2_KEY_PATH = r"C:\Users\heman\Downloads\key-scraper.pem"` | `EC2_KEY_PATH = r"C:\path\to\your-key.pem"` | `web_server.py` |
| `S3_BUCKET_NAME=bockscraper` | `S3_BUCKET_NAME=your-bucket` | `.env` |
| `echo Checking connection to 54.82.140.246...` | `echo Checking connection to YOUR_NEW_IP...` | `launch_scraper_interface.bat` |

### **‚ö†Ô∏è Critical Notes for Deployment Teams**

1. **Always update BOTH files for EC2 changes** (`web_server.py` and `launch_scraper_interface.bat`)
2. **Test connections** after each change using the web interface
3. **Keep backup copies** of configuration files before making changes
4. **Use absolute paths** for SSH key files (avoid relative paths)
5. **Ensure EC2 security groups** allow SSH access from your IP

## 6. üîÑ **STEP-BY-STEP: Changing EC2 Instance or S3 Bucket**

**This section provides detailed, file-by-file instructions for deployment teams to change infrastructure.**

---

### 6.1 üñ•Ô∏è **Changing EC2 Instance - Complete Code Changes**

**When switching to a different EC2 instance, you need to update 3 files:**

#### **Step 1: Update `web_server.py` (Lines 30-32)**
```python
# FIND THIS CODE (Lines 30-32):
EC2_HOST = "54.82.140.246"                                    # ‚ö†Ô∏è CHANGE THIS LINE
EC2_USER = "ec2-user"                                         # ‚úÖ Usually keep as-is
EC2_KEY_PATH = r"C:\Users\heman\Downloads\key-scraper.pem"    # ‚ö†Ô∏è CHANGE THIS LINE

# REPLACE WITH YOUR VALUES:
EC2_HOST = "YOUR_NEW_EC2_IP_HERE"                             # üëà New EC2 public IP
EC2_USER = "ec2-user"                                         # ‚úÖ Keep same user
EC2_KEY_PATH = r"C:\path\to\your\new-ec2-key.pem"           # üëà New key file path
```

#### **Step 2: Update `launch_scraper_interface.bat` (Line 66)**
```batch
REM FIND THIS LINE (Line 66):
echo Checking connection to 54.82.140.246...                # ‚ö†Ô∏è CHANGE THIS

REM REPLACE WITH:
echo Checking connection to YOUR_NEW_EC2_IP_HERE...         # üëà Same IP as above
```

#### **Step 3: Setup New EC2 Instance**

#### **Step 1: Update Configuration Files**
```bash
# 1. Update web_server.py (lines 30-32)
EC2_HOST = "NEW_EC2_IP_ADDRESS"
EC2_KEY_PATH = r"C:\path\to\new-key.pem"

# 2. Update launch_scraper_interface.bat (line 66)
echo Checking connection to NEW_EC2_IP_ADDRESS...
```

#### **Step 2: Setup New EC2 Instance**
```bash
# Connect to new EC2 instance
ssh -i "C:\path\to\new-key.pem" ec2-user@NEW_EC2_IP

# Install dependencies (follow Section 2.1.B)
sudo yum update -y
sudo yum install -y python3 python3-pip python3-devel gcc awscli
sudo yum install -y libjpeg-devel zlib-devel freetype-devel

# Upload scraper script
scp -i "C:\path\to\new-key.pem" ultimate_scraper_v2.py ec2-user@NEW_EC2_IP:~/

# Create virtual environment
python3 -m venv ~/ultimate_scraper_env
source ~/ultimate_scraper_env/bin/activate
pip install scrapy trafilatura beautifulsoup4 lxml newspaper3k Pillow aiohttp requests

# Configure AWS CLI (if using access keys)
aws configure
```

#### **Step 3: Test New EC2 Connection**
```bash
# Test from final folder
cd final
python web_server.py
# Click "Test EC2 Connection" in web interface
```

### 6.2 Changing S3 Bucket

If you need to switch to a different S3 bucket:

#### **Step 1: Create New S3 Bucket**
```bash
# Create new bucket
aws s3 mb s3://your-new-bucket-name --region us-east-1

# Enable versioning (optional)
aws s3api put-bucket-versioning \
  --bucket your-new-bucket-name \
  --versioning-configuration Status=Enabled
```

#### **Step 2: Update Configuration Files**
```bash
# 1. Update .env file
S3_BUCKET_NAME=your-new-bucket-name

# 2. Update web_server.py (line 37)
S3_BUCKET_NAME = os.getenv('S3_BUCKET_NAME', 'your-new-bucket-name')
```

#### **Step 3: Update IAM Permissions**
```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:PutObject",
                "s3:PutObjectAcl",
                "s3:GetObject",
                "s3:ListBucket",
                "s3:DeleteObject"
            ],
            "Resource": [
                "arn:aws:s3:::your-new-bucket-name",
                "arn:aws:s3:::your-new-bucket-name/*"
            ]
        }
    ]
}
```

#### **Step 4: Test S3 Access**
```bash
# Test bucket access
aws s3 ls s3://your-new-bucket-name
aws s3 cp test-file.txt s3://your-new-bucket-name/test/

# Test via web interface
# Run a small scraping job and verify S3 upload works
```

### 6.3 Quick Configuration Summary

**For EC2 Change:**
1. Update `web_server.py` (2 lines)
2. Update `launch_scraper_interface.bat` (1 line)  
3. Setup new EC2 instance
4. Test connection

**For S3 Change:**
1. Create new S3 bucket
2. Update `.env` file (1 line)
3. Update `web_server.py` (1 line)
4. Update IAM permissions
5. Test S3 access

**Both are completely independent** - you can change EC2 without touching S3, or change S3 without touching EC2.
